# üì∞ News Scraper - Django REST Framework + Frontend

Projeto profissional de Web Scraping com **Django**, **Django REST Framework** e **Frontend JavaScript**. Sistema completo de extra√ß√£o, armazenamento e visualiza√ß√£o de not√≠cias.

## ‚ö° In√≠cio R√°pido

### üê≥ Com Docker (Recomendado)

```bash
# 1. Iniciar com Docker Compose
docker-compose up -d

# 2. Acessar aplica√ß√£o
Frontend: http://localhost:8000/frontend/index.html
API: http://localhost:8000/api/noticias/
Admin: http://localhost:8000/admin/ (admin/admin123)

# 3. Fazer scraping de not√≠cias
docker-compose exec web python manage.py scrape_news

# 4. Ver logs
docker-compose logs -f web

# 5. Parar containers
docker-compose down
```

### üíª Instala√ß√£o Manual

```bash
# 1. Instalar depend√™ncias
pip install -r requirements.txt

# 2. Configurar banco de dados
python manage.py migrate

# 3. Criar superusu√°rio (acesso ao admin)
python manage.py createsuperuser

# 4. Fazer scraping de not√≠cias
python manage.py scrape_news

# 5. Iniciar servidor
python manage.py runserver

# 6. Acessar aplica√ß√£o
Frontend: http://localhost:8000/frontend/index.html
API: http://localhost:8000/api/noticias/
Admin: http://localhost:8000/admin/
```

## üéØ Objetivo

Sistema completo de web scraping que:
- Extrai not√≠cias de sites brasileiros automaticamente
- Armazena dados em banco de dados Django (PostgreSQL ou SQLite)
- Disponibiliza API REST completa (DRF)
- Interface frontend moderna consumindo a API
- Painel administrativo Django customizado

## ‚ú® Funcionalidades

### Backend (Django + DRF)
- ‚úÖ **API REST completa** - CRUD de not√≠cias
- ‚úÖ **Django REST Framework** - Serializers e ViewSets
- ‚úÖ **PostgreSQL/SQLite** - Banco de dados configur√°vel
- ‚úÖ **Docker + Docker Compose** - Containeriza√ß√£o completa
- ‚úÖ **Admin Django customizado** - Gerenciamento visual
- ‚úÖ **Management Commands** - Automa√ß√£o de scraping
- ‚úÖ **Pagina√ß√£o autom√°tica** - Listagem otimizada
- ‚úÖ **CORS configurado** - Pronto para produ√ß√£o
- ‚úÖ **Endpoints de estat√≠sticas** - An√°lise de dados
- ‚úÖ **Vari√°veis de ambiente** - Configura√ß√£o segura

### Frontend (HTML/CSS/JavaScript)
- ‚úÖ **Interface moderna e responsiva**
- ‚úÖ **Consumo de API REST** com Fetch API
- ‚úÖ **Grid de not√≠cias** com anima√ß√µes CSS
- ‚úÖ **Bot√µes de a√ß√£o** (recarregar, limpar, estat√≠sticas)
## üì¶ Depend√™ncias

### Backend
- **Python 3.13+**
- **Django 5.0** - Framework web full-featured
- **Django REST Framework 3.14** - API RESTful
- **django-cors-headers** - Configura√ß√£o CORS
- **PostgreSQL** - Banco de dados (produ√ß√£o) - opcional
- **SQLite** - Banco de dados (desenvolvimento) - padr√£o
- **psycopg2-binary** - Driver PostgreSQL
- **python-dotenv** - Vari√°veis de ambiente
- **BeautifulSoup4 4.12** - Parse de HTML
- **Requests 2.31** - Requisi√ß√µes HTTP

### Infraestrutura
- **Docker** - Containeriza√ß√£o
- **Docker Compose** - Orquestra√ß√£o de containers
- **PostgreSQL 16** (Alpine) - Banco de dados em container

### Frontend
- **HTML5** - Estrutura sem√¢ntica
- **CSS3** - Grid, Flexbox, anima√ß√µes e gradientes
- **JavaScript (ES6+)** - Async/await, Fetch API
- **Vanilla JS** - Sem depend√™ncias de framework

## üõ†Ô∏è Tecnologias Utilizadas

### Backend
- **Python 3.13+**
- **BeautifulSoup4** - Parse de HTML
- **Requests** - Requisi√ß√µes HTTP
- **CSV & JSON** - Manipula√ß√£o de dados

### Frontend
- **HTML5** - Estrutura da p√°gina
- **CSS3** - Estiliza√ß√£o e anima√ß√µes
- **JavaScript (Vanilla)** - Interatividade e carregamento de dados

- Navegador web moderno

## üöÄ Instala√ß√£o Detalhada
- Python 3.7 ou superior
- pip (gerenciador de pacotes Python)

## üöÄ Como Usar

### 1. Clone o reposit√≥rio
```bash
git clone <seu-repositorio>
cd "News Scraper"
```

### 2. Crie um ambiente virtual (recomendado)
```bash
python -m venv venv
```

### 3. Ative o ambiente virtual
**Windows:**
```bash
venv\Scripts\activate
```

**Linux/Mac:**
```bash
source venv/bin/activate
```

### 4. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

### 5. Configure o banco de dados

**Op√ß√£o A: SQLite (padr√£o, mais f√°cil)**
```bash
# Nenhuma configura√ß√£o adicional necess√°ria
# O arquivo db.sqlite3 ser√° criado automaticamente
```

**Op√ß√£o B: PostgreSQL (produ√ß√£o)**
```bash
# 1. Instale PostgreSQL no seu sistema
# 2. Crie um banco de dados
# 3. Copie o arquivo de exemplo
copy .env.example .env  # Windows
cp .env.example .env    # Linux/Mac

# 4. Edite o arquivo .env com suas credenciais:
# DB_NAME=news_scraper_db
# DB_USER=seu_usuario
# DB_PASSWORD=sua_senha
# DB_HOST=localhost
# DB_PORT=5432
# USE_SQLITE=False
```

### 6. Execute as migra√ß√µes
### 6. Execute as migra√ß√µes
```bash
python manage.py migrate
```

### 7. Crie um superusu√°rio (para acessar o admin)
```bash
python manage.py createsuperuser
# Siga as instru√ß√µes: username, email, password
```

**Credenciais de teste configuradas:**
- **Usu√°rio**: `teste`
- **Senha**: `teste123`

### 8. Execute o scraping inicial
```bash
python manage.py scrape_news
```

### 9. Inicie o servidor Django
```bash
python manage.py runserver
```

### 10. Acesse a aplica√ß√£o
- **Frontend**: http://localhost:8000/frontend/index.html
- **API REST**: http://localhost:8000/api/noticias/
- **Admin Django**: http://localhost:8000/admin/ (user: `teste` | senha: `teste123`)

## üêò Configura√ß√£o PostgreSQL (Opcional)

### Windows
```bash
# 1. Baixe e instale PostgreSQL: https://www.postgresql.org/download/windows/

# 2. Abra pgAdmin ou use psql para criar banco:
createdb news_scraper_db

# 3. Configure vari√°veis de ambiente no .env
```

### Linux (Ubuntu/Debian)
```bash
# 1. Instale PostgreSQL
sudo apt update
sudo apt install postgresql postgresql-contrib

# 2. Crie usu√°rio e banco
sudo -u postgres psql
CREATE DATABASE news_scraper_db;
CREATE USER seu_usuario WITH PASSWORD 'sua_senha';
ALTER ROLE seu_usuario SET client_encoding TO 'utf8';
ALTER ROLE seu_usuario SET default_transaction_isolation TO 'read committed';
ALTER ROLE seu_usuario SET timezone TO 'America/Sao_Paulo';
GRANT ALL PRIVILEGES ON DATABASE news_scraper_db TO seu_usuario;
\q

# 3. Configure o arquivo .env
```

### macOS
```bash
# 1. Instale PostgreSQL via Homebrew
brew install postgresql
brew services start postgresql

# 2. Crie banco de dados
createdb news_scraper_db

# 3. Configure o arquivo .env
```

## üîí Vari√°veis de Ambiente

O projeto usa um arquivo `.env` para configura√ß√µes sens√≠veis. Copie o `.env.example`:

```bash
copy .env.example .env  # Windows
cp .env.example .env    # Linux/Mac
```

Edite o `.env` conforme necess√°rio:

```env
# Database (PostgreSQL)
DB_NAME=news_scraper_db
DB_USER=postgres
DB_PASSWORD=postgres
DB_HOST=localhost
DB_PORT=5432

# Use SQLite ao inv√©s de PostgreSQL
USE_SQLITE=False  # True para desenvolvimento com SQLite

# Django
DEBUG=True
SECRET_KEY=sua-chave-secreta-aqui
```

**‚ö†Ô∏è IMPORTANTE**: Nunca commite o arquivo `.env` no Git! Ele j√° est√° no `.gitignore`.

## üìÇ Estrutura do Projeto

```
News Scraper/
‚îú‚îÄ‚îÄ config/                 # Configura√ß√µes Django
‚îÇ   ‚îú‚îÄ‚îÄ settings.py        # Configura√ß√µes (PostgreSQL/SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ urls.py            # URLs principais
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py            # WSGI config
‚îÇ
‚îú‚îÄ‚îÄ news/                   # App Django principal
‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Model Noticia
‚îÇ   ‚îú‚îÄ‚îÄ serializers.py     # Serializers DRF
‚îÇ   ‚îú‚îÄ‚îÄ views.py           # ViewSets API
‚îÇ   ‚îú‚îÄ‚îÄ urls.py            # URLs da API
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Admin customizado
‚îÇ   ‚îú‚îÄ‚îÄ migrations/        # Migra√ß√µes do banco
‚îÇ   ‚îî‚îÄ‚îÄ management/        # Comandos customizados
‚îÇ       ‚îî‚îÄ‚îÄ commands/
‚îÇ           ‚îî‚îÄ‚îÄ scrape_news.py
‚îÇ
‚îú‚îÄ‚îÄ frontend/              # Frontend JavaScript
‚îÇ   ‚îú‚îÄ‚îÄ index.html        # Interface principal
‚îÇ   ‚îú‚îÄ‚îÄ styles.css        # Estilos CSS
‚îÇ   ‚îî‚îÄ‚îÄ app.js            # L√≥gica JavaScript
‚îÇ
‚îú‚îÄ‚îÄ .env.example          # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ .gitignore            # Arquivos ignorados pelo Git
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias Python
‚îú‚îÄ‚îÄ manage.py             # CLI Django
‚îî‚îÄ‚îÄ README.md             # Documenta√ß√£o
```

## ‚öôÔ∏è Comandos Django √öteis

### Scraping de Not√≠cias
```bash
# Fazer scraping padr√£o (G1)
python manage.py scrape_news

# Limpar banco e fazer scraping
python manage.py scrape_news --limpar

# Scraping de URL customizada
python manage.py scrape_news --url https://outro-site.com
```

### Gerenciamento do Banco
```bash
# Criar migra√ß√µes
python manage.py makemigrations

# Aplicar migra√ß√µes
python manage.py migrate

# Shell interativo Django
python manage.py shell

# Criar superusu√°rio
python manage.py createsuperuser
```

## üì° API Endpoints

| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| GET | `/api/noticias/` | Lista todas not√≠cias (paginado) |
| POST | `/api/noticias/` | Cria nova not√≠cia |
| GET | `/api/noticias/{id}/` | Detalhes de uma not√≠cia |
| PUT | `/api/noticias/{id}/` | Atualiza not√≠cia |
| DELETE | `/api/noticias/{id}/` | Deleta not√≠cia |
| DELETE | `/api/noticias/limpar_tudo/` | Deleta todas not√≠cias |
| GET | `/api/noticias/estatisticas/` | Estat√≠sticas do banco |

### Exemplo de Requisi√ß√£o
```bash
# Listar not√≠cias
curl http://localhost:8000/api/noticias/

# Obter estat√≠sticas
curl http://localhost:8000/api/noticias/estatisticas/
```

### Exemplo de Resposta JSON
```json
{
    "count": 8,
    "next": null,
    "previous": null,
    "results": [
        {
            "id": 1,
            "titulo": "T√≠tulo da Not√≠cia",
            "descricao": "Descri√ß√£o...",
            "link": "https://g1.globo.com/...",
            "data_extracao": "2026-01-02T23:10:00Z",
            "fonte": "G1"
        }
    ]
}. Clique com bot√£o direito > Inspecionar
3. Identifique as tags HTML e classes das not√≠cias
4. Atualize os seletores no c√≥digo

## üìä Formato dos Dados

### CSV
```csv
id,titulo,descricao,link,data_extracao
1,"T√≠tulo da Not√≠cia","Descri√ß√£o breve...","https://...","2026-01-02 10:30:00"
```

### JSON
```json
[
    {
        "id": 1,
        "titulo": "T√≠tulo da Not√≠cia",
        "descricao": "Descri√ß√£o breve...",
        "link": "https://...",
        "data_extracao": "2026-01-02 10:30:00"
    }
]
```

## üê≥ Docker

### Por que Docker?

- **Portabilidade**: Funciona em qualquer m√°quina com Docker
- **Consist√™ncia**: Mesmo ambiente em desenvolvimento e produ√ß√£o
- **Isolamento**: N√£o interfere com outras instala√ß√µes
- **Facilidade**: Um comando para subir tudo

### Pr√©-requisitos

- [Docker Desktop](https://www.docker.com/products/docker-desktop/) instalado
- Docker Compose (inclu√≠do no Docker Desktop)

### Comandos Docker

```bash
# Iniciar containers (primeira vez ou ap√≥s mudan√ßas)
docker-compose up --build

# Iniciar em background
docker-compose up -d

# Ver logs em tempo real
docker-compose logs -f
docker-compose logs -f web    # Apenas da aplica√ß√£o
docker-compose logs -f db     # Apenas do PostgreSQL

# Executar comandos dentro do container
docker-compose exec web python manage.py scrape_news
docker-compose exec web python manage.py createsuperuser
docker-compose exec web python manage.py shell

# Acessar terminal do container
docker-compose exec web bash
docker-compose exec db psql -U postgres -d news_scraper_db

# Parar containers (mant√©m dados)
docker-compose stop

# Parar e remover containers
docker-compose down

# Parar, remover E deletar volumes (CUIDADO: perde dados!)
docker-compose down -v

# Rebuild ap√≥s mudan√ßas no c√≥digo
docker-compose up --build

# Ver status dos containers
docker-compose ps

# Ver uso de recursos
docker stats
```

### Estrutura Docker

```
News Scraper/
‚îú‚îÄ‚îÄ Dockerfile              # Imagem da aplica√ß√£o Django
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o (Django + PostgreSQL)
‚îú‚îÄ‚îÄ .dockerignore          # Arquivos ignorados no build
‚îî‚îÄ‚îÄ entrypoint.sh          # Script de inicializa√ß√£o
```

### Volumes Docker

Os dados do PostgreSQL s√£o persistidos em um volume Docker:
- **postgres_data**: Banco de dados (n√£o √© perdido ao parar containers)

### Vari√°veis de Ambiente Docker

Configuradas no `docker-compose.yml`:
- `DEBUG=True` - Modo debug (mudar para False em produ√ß√£o)
- `DB_HOST=db` - Nome do servi√ßo PostgreSQL no Docker
- `DB_NAME=news_scraper_db` - Nome do banco
- `DB_USER=postgres` - Usu√°rio PostgreSQL
- `DB_PASSWORD=postgres` - Senha PostgreSQL

### Troubleshooting Docker

**Erro de porta j√° em uso:**
```bash
# Altere a porta no docker-compose.yml
ports:
  - "8001:8000"  # Usar porta 8001 ao inv√©s de 8000
```

**Container n√£o inicia:**
```bash
# Ver logs detalhados
docker-compose logs web

# Rebuild for√ßado
docker-compose build --no-cache
docker-compose up
```

**Resetar tudo:**
```bash
# CUIDADO: Apaga TUDO (dados, volumes, containers)
docker-compose down -v
docker-compose up --build
```

## ‚ö†Ô∏è Considera√ß√µes Legais e √âticas
utentica√ß√£o JWT para API
- [ ] Testes automatizados (pytest, unittest)
- [ ] Deploy em produ√ß√£o (Heroku, Railway, AWS)
- [ ] PostgreSQL em produ√ß√£o
- [ ] Docker e docker-compose
- [ ] Celery para scraping ass√≠ncrono
- [ ] Agendamento de scraping (Celery Beat)
- [ ] Frontend em React ou Vue.js
- [ ] WebSockets para atualiza√ß√µes em tempo real
- [ ] An√°lise de sentimentos com NLP
- [ ] Cache com Redis
- [ ] Integra√ß√£o com m√∫ltiplas fontes de not√≠ciacacionais

## üéì Habilidades DemonstradasFull-Stack demonstrando:
- Backend Django + Django REST Framework
- Frontend JavaScript puro (Vanilla JS)
- Web Scraping profissional
- Integra√ß√£o completa de sistemas

Ideal para vagas de **Desenvolvedor Python J√∫nior/Pleno** e **Desenvolvedor Full-Stack**.

---

‚≠ê **Nota para Recrutadores**: 
- Sistema completo e funcional
- C√≥digo limpo e documentado
- Arquitetura profissional
- Boas pr√°ticas da ind√∫stria
- Pronto para produ√ß√£o (com ajustes)

üìß Contato para oportunidades de trabalho!
### Backend
- Programa√ß√£o Orientada a Objetos (OOP)
- Manipula√ß√£o de requisi√ß√µes HTTP
- Parse e an√°lise de HTML
- Tratamento de exce√ß√µes
- Manipula√ß√£o de arquivos
- Boas pr√°ticas de Python (PEP 8)

### Frontend
- HTML5 sem√¢ntico
- CSS3 com Flexbox e Grid
- JavaScript ass√≠ncrono (async/await)
- Fetch API
- Manipula√ß√£o do DOM
- Design responsivo
- Anima√ß√µes CSS

### Geral
- Documenta√ß√£o de c√≥digo
- Integra√ß√£o Frontend-Backend
- Desenvolvimento Full-Stack b√°sico
- Boas pr√°ticas de Python (PEP 8)

## ÔøΩ Deploy na AWS EC2

### Pr√©-requisitos

- Conta AWS ativa
- EC2 instance (Ubuntu 22.04 LTS recomendado)
- Par de chaves SSH (.pem)
- Security Group com portas abertas: 22 (SSH), 80 (HTTP), 443 (HTTPS), 8000 (Django)

### Passo a Passo

#### 1. Conectar ao EC2

```bash
# Alterar permiss√µes da chave
chmod 400 sua-chave.pem

# Conectar via SSH
ssh -i sua-chave.pem ubuntu@seu-ec2-ip
```

#### 2. Instalar Depend√™ncias no Servidor

```bash
# Atualizar sistema
sudo apt update && sudo apt upgrade -y

# Instalar Python e depend√™ncias
sudo apt install -y python3-pip python3-venv postgresql postgresql-contrib nginx git

# Instalar Docker (opcional, para usar containers)
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker ubuntu
```

#### 3. Clonar Projeto

```bash
# Criar diret√≥rio
mkdir -p ~/apps
cd ~/apps

# Clonar reposit√≥rio
git clone https://github.com/seu-usuario/news-scraper.git
cd news-scraper
```

#### 4. Configurar PostgreSQL

```bash
# Acessar PostgreSQL
sudo -u postgres psql

# Dentro do psql:
CREATE DATABASE news_scraper_db;
CREATE USER news_user WITH PASSWORD 'senha_segura_aqui';
ALTER ROLE news_user SET client_encoding TO 'utf8';
ALTER ROLE news_user SET default_transaction_isolation TO 'read committed';
ALTER ROLE news_user SET timezone TO 'America/Sao_Paulo';
GRANT ALL PRIVILEGES ON DATABASE news_scraper_db TO news_user;
\q
```

#### 5. Configurar Ambiente Python

```bash
# Criar ambiente virtual
python3 -m venv venv
source venv/bin/activate

# Instalar depend√™ncias
pip install --upgrade pip
pip install -r requirements.txt
```

#### 6. Configurar Vari√°veis de Ambiente

```bash
# Criar arquivo .env
nano .env

# Adicionar (pressione Ctrl+X, Y, Enter para salvar):
DEBUG=False
SECRET_KEY=$(python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())')
ALLOWED_HOSTS=seu-dominio.com,www.seu-dominio.com,seu-ec2-ip,localhost,127.0.0.1
FORCE_SCRIPT_NAME=/news-scraper
DB_NAME=news_scraper_db
DB_USER=news_user
DB_PASSWORD=senha_segura_aqui
DB_HOST=localhost
DB_PORT=5432
USE_SQLITE=False
```

**‚ö†Ô∏è IMPORTANTE**: Ajustar permiss√µes do diret√≥rio home para o Nginx acessar arquivos est√°ticos:

```bash
chmod 755 /home/ubuntu
chmod 755 /home/ubuntu/apps
chmod 755 /home/ubuntu/apps/news-scraper
chmod -R 755 ~/apps/news-scraper/staticfiles/
```

#### 7. Preparar Django

```bash
# Executar migra√ß√µes
python manage.py migrate

# Criar superusu√°rio
python manage.py createsuperuser

# Coletar arquivos est√°ticos
python manage.py collectstatic --noinput

# Testar servidor
python manage.py runserver 0.0.0.0:8000
# Acesse http://seu-ec2-ip:8000
# Pressione Ctrl+C para parar
```

#### 8. Configurar Gunicorn como Servi√ßo

```bash
# Criar arquivo de servi√ßo systemd
sudo nano /etc/systemd/system/news-scraper.service
```

Adicione:

```ini
[Unit]
Description=News Scraper Gunicorn daemon
After=network.target

[Service]
User=ubuntu
Group=www-data
WorkingDirectory=/home/ubuntu/apps/news-scraper
Environment="PATH=/home/ubuntu/apps/news-scraper/venv/bin"
EnvironmentFile=/home/ubuntu/apps/news-scraper/.env
ExecStart=/home/ubuntu/apps/news-scraper/venv/bin/gunicorn \
          --config gunicorn_config.py \
          config.wsgi:application

[Install]
WantedBy=multi-user.target
```

```bash
# Habilitar e iniciar servi√ßo
sudo systemctl daemon-reload
sudo systemctl start news-scraper
sudo systemctl enable news-scraper
sudo systemctl status news-scraper
```

#### 9. Configurar Nginx

```bash
# Criar configura√ß√£o do site
sudo nano /etc/nginx/sites-available/news-scraper
```

Adicione:

```nginx
server {
    listen 80 default_server;
    server_name seu-dominio.com www.seu-dominio.com seu-ec2-ip;

    location = /favicon.ico { access_log off; log_not_found off; }
    
    location /news-scraper/static/ {
        alias /home/ubuntu/apps/news-scraper/staticfiles/;
    }

    location /news-scraper/ {
        proxy_pass http://127.0.0.1:8001/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

```bash
# Ativar site
sudo ln -s /etc/nginx/sites-available/news-scraper /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx
```

#### 10. Configurar HTTPS com Let's Encrypt (Opcional mas Recomendado)

```bash
# Instalar Certbot
sudo apt install -y certbot python3-certbot-nginx

# Obter certificado SSL
sudo certbot --nginx -d seu-dominio.com -d www.seu-dominio.com

# Renova√ß√£o autom√°tica (j√° configurado pelo Certbot)
sudo systemctl status certbot.timer
```

### Deploy com Docker (Alternativa)

```bash
# No servidor EC2
cd ~/apps/news-scraper

# Build e start
docker-compose -f docker-compose.prod.yml up -d --build

# Ver logs
docker-compose logs -f web
```

### Comandos √öteis no Servidor

```bash
# Ver logs do Gunicorn
sudo journalctl -u news-scraper -f

# Reiniciar servi√ßo
sudo systemctl restart news-scraper

# Ver logs do Nginx
sudo tail -f /var/log/nginx/access.log
sudo tail -f /var/log/nginx/error.log

# Atualizar c√≥digo
cd ~/apps/news-scraper
git pull origin main
source venv/bin/activate
pip install -r requirements.txt
python manage.py migrate
python manage.py collectstatic --noinput
sudo systemctl restart news-scraper
```

### Seguran√ßa Adicional

```bash
# Configurar firewall
sudo ufw allow 22/tcp
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw enable

# Desabilitar acesso direto √† porta 8000
# (Nginx faz proxy reverso)
```

### Backup do Banco de Dados

```bash
# Exportar banco
pg_dump -U news_user news_scraper_db > backup_$(date +%Y%m%d).sql

# Restaurar banco
psql -U news_user news_scraper_db < backup_20260103.sql
```

### Monitoramento

```bash
# Instalar htop para monitorar recursos
sudo apt install -y htop
htop

# Ver processos do Gunicorn
ps aux | grep gunicorn

# Ver uso de mem√≥ria
free -h

# Ver uso de disco
df -h
```

### Troubleshooting

**Erro 502 Bad Gateway:**
```bash
# Verificar status do Gunicorn
sudo systemctl status news-scraper

# Ver logs
sudo journalctl -u news-scraper -n 50
```

**Static files n√£o carregam:**
```bash
# Coletar novamente
source venv/bin/activate
python manage.py collectstatic --noinput
sudo systemctl restart nginx
```

**Erro de conex√£o com banco:**
```bash
# Verificar PostgreSQL
sudo systemctl status postgresql

# Testar conex√£o
psql -U news_user -d news_scraper_db -h localhost
```

## ÔøΩüîÑ Melhorias Futuras

- [ ] Adicionar suporte a m√∫ltiplas p√°ginas (pagina√ß√£o)
- [ ] Implementar banco de dados (SQLite/PostgreSQL)
- [ ] Criar interface gr√°fica (GUI)
- [ ] Adicionar agendamento autom√°tico
- [ ] Implementar an√°lise de sentimentos
- [ ] Dashboard de visualiza√ß√£o de dados

## üìù Licen√ßa

Projeto de uso livre para fins educacionais e de portfolio.

## üë§ Autor

Desenvolvido como projeto portfolio para vaga de desenvolvedor j√∫nior.

---

‚≠ê Se este projeto foi √∫til, considere dar uma estrela no reposit√≥rio!
